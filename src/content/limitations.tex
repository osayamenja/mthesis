%! Author = jonathan
%! Date = 5/27/25
\chapter{Limitations and Future Work}\label{ch:limitations-and-future-work}
Despite the performance gains and architectural innovations of \sysname, there are several limitations
worth acknowledging---both practical and conceptual---that open the door to future research.
\begin{itemize}
    \item \textbf{Programming complexity.} Developing fully fused, persistent kernels is a non-trivial engineering task.
    While \sysname proves the feasibility and benefit of such kernels,
    their construction demands deep expertise in GPU architectures,
    synchronization and distributed protocols, and memory hierarchies.
    This high barrier to entry limits adoption.
    Future work may consider compiler-level abstractions or DSLs to democratize this technique.

    \item \textbf{FP16 support and shared memory bank conflicts.}
    Although modern GPUs natively support half-precision computation,
    adapting \textsc{FlashDMoE} to FP16 is non-trivial for the Processor's computational operators.
    Specifically, our manually tuned swizzle shared memory layouts are not the most efficient
    template parameters for CUTLASS' Collective Mainloop operator which we use to implement our in-device GEMMs.
    This suboptimal configuration degrades memory throughput as shown in \S\ref{ch:fp16-memory-throughput}.
    Overcoming this for Ampere GPUs and below would require careful investigation of optimal layouts, but
    for Hopper GPUs and above, we anticipate using the builder interface that CUTLASS provides in our future improvements.

    \item \textbf{Lack of backward pass and training support.} While this work focuses on inference,
    enabling training requires fusing backward computation and gradient communication into the kernel.
    Supporting this entails non-trivial changes to both memory bookkeeping and task descriptor definitions.
    Nevertheless, it remains an exciting direction for extending this system to
    fully support end-to-end training.
\end{itemize}


